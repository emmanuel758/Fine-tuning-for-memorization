# Fine-tuning-for-memorization
This repository contains the code and resources for fine-tuning a GPT-2 model to improve its ability to respond based on a custom dataset. The goal is to enhance the model's memorization capabilities and integrate it into a web application via APIs. The fine-tuning process focuses on adjusting the model to provide accurate and context-aware responses, specifically tailored to the input data.

## Features:
1. Fine-tuning GPT-2 on custom Q&A datasets.  
2. API integration using FastAPI for generating responses.  
3. Easy deployment for web applications.  
